---
title: "[논문 리뷰] Attention Is All You Need[2]: Attention"
excerpt: "자연어 분야의 핵심 논문 중 하나인 Attention Is All You Need을 리뷰한다. 어텐션 알고리즘과 모델 아키택처를 그림과 코드를 통해 이해하고, 병렬성 관점에서 살펴본다." # 주요 내용

categories:
  - NLP
  - Paper
tags:
  - [NLP, Paper]

permalink: /Paper/Attention2/

toc: true
toc_sticky: true

# date: 2024-05-08
# last_modified_at: 2024-05-20

author_profile: true
search: true
use_math: true
---

## Attention

- Attention이란 query를 key-value 쌍에 매핑하는 것
- query와 key의 유사도를 구해 가중치를 구함
- 이를 해당 key의 value 값 가중치로 사용
- output 값은 value의 가중치 합

<div style="display: flex; justify-content: space-around;">
    <img src="{{site.url}}/assets/images/posts_img/2024-05-20-1/SelfAttention.png" alt="Attention" style="width: 100%;"/>
</div>

### Scaled Dot-Product Attention
<div align="center">
$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
</div>

- 쿼리(Q), 키(K) 벡터들의 도트 프로덕트를 $ \sqrt{d_k} $로 나눈 후 소프트맥스 적용하여 가중치 구함
- 값(V)들의 가중합으로 어텐션 출력값 계산
- $ \sqrt{d_k} $은 큰 $ \d_k $ 값에서 소프트맥스 함수의 작은 그래디언트 문제 해결

### Multi-Head Attention

- 쿼리, 키, 값을 각각 서로 다른 선형 변환하여 h개의 병렬 어텐션 계산
- 각 어텐션 헤드는 Scaled Dot-Product Attention 수행
- 모든 헤드의 출력값들을 연결하고 최종적으로 선형 변환
- 다중 헤드를 통해 서로 다른 위치에서 다양한 정보에 주목 가능 

## 레퍼런스

1. [Pytorch 공식 코드](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer) 
2. [Transformer 분석: Transformer의 Encoder 이해하기](https://moon-walker.medium.com/transformer-%EB%B6%84%EC%84%9D-2-transformer%EC%9D%98-encoder-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-1edecc2ad5d4)