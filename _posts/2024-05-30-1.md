---
title: "[머신러닝] MLE 기술 면접 대비 질문 - 딥러닝"
excerpt: "수학, 통계, 머신러닝, 딥러닝 기술 면접을 위한 문제와 답변 해보기" # 주요 내용

categories:
  - Machine Learning
tags:
  - [Machine Learning]

permalink: /Machine Learning/MLE기술면접대비질문2/

toc: true
toc_sticky: true

date: 2024-05-30
# last_modified_at: 2024-05-20

author_profile: true
search: true
use_math: true
---

## 딥러닝 분야에서 Non-linearlty란?
딥러닝에서 비선형성이란 신경망이 복잡한 패턴을 학습할 수 있도록 하는 중요한 개념.
선형 모델은 데이터간 선형 관계만 학습할 수 있다면, 비선형 모델은 복잡하고 다차원의 관계를 학습할 수 있음. 비선형성은 주로 활성화 함수를 통해 도입.

- ReLU
  <div align="center">
  $$ f(x) = \max(0, x) $$
  </div>
  가장 많이 사용되는 activation function으로, gradient vanishing 문제를 완화.

- Sigmoid
  <div align="center">
  $$ f(x) = \frac{1}{1 + e^{-x}} $$
  </div>
  출력 값이 0~1 사이라 이진 문제에서 주로 사용. 기울기가 0~0.25로 매우 작아 gradient vanishing 문제가 발생.

- Tanh
  <div align="center">
  $$ f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$
  </div>
  출력 값이 -1~1 사이. 기울기가 0~1 사이로 vanishing 문제가 sigmoid보단 덜하지만 있음.

- Leaky ReLU
  <div align="center">
  $$ f(x) = \begin{cases} x & \text{if } x > 0 \\
  \alpha x & \text{if } x \leq 0
  \end{cases}
  $$
  </div>
  ReLU의 변형. 음수 입력에 대해 작은 기울기를 허용하여 죽은 뉴런 문제 완화.