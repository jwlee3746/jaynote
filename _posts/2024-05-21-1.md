---
title: "[논문 리뷰] Attention Is All You Need[2]: Feed-Forward, Positional Encoding in Transformer"
excerpt: "자연어 분야의 핵심 논문 중 하나인 Attention Is All You Need을 리뷰한다. 어텐션 알고리즘과 모델 아키택처를 그림과 코드를 통해 이해하고, 병렬성 관점에서 살펴본다." # 주요 내용

categories:
  - NLP
  - Paper
tags:
  - [NLP, Paper]

permalink: /Paper/Attention3/

toc: true
toc_sticky: true

date: 2024-05-21
# last_modified_at: 2024-05-20

author_profile: true
search: true
use_math: true
---

## Position-wise Feed-Forward Networks

두 개의 선형 변환 사이에 ReLU 활성화 함수 사용
모든 위치에 동일하게 적용되지만 레이어마다 다른 파라미터 사용
입출력 차원 512, 내부 차원 2048

요컨데 Transformer는 셀프 어텐션, 인코더-디코더 어텐션, 피드포워드 네트워크를 조합하여 입출력 시퀀스를 효과적으로 처리합니다.


## 레퍼런스

1. [Pytorch 공식 코드](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer) 
2. [Transformer 분석: Transformer의 Encoder 이해하기](https://moon-walker.medium.com/transformer-%EB%B6%84%EC%84%9D-2-transformer%EC%9D%98-encoder-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-1edecc2ad5d4)