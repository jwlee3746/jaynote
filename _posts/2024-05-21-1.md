---
title: "[논문 리뷰] Attention Is All You Need[3]: Feed-Forward, Positional Encoding"
excerpt: "자연어 분야의 핵심 논문 중 하나인 Attention Is All You Need을 리뷰한다. 어텐션 알고리즘과 모델 아키택처를 그림과 코드를 통해 이해하고, 병렬성 관점에서 살펴본다." # 주요 내용

categories:
- - NLP
- - Paper
tags:
  - [NLP, Paper]

permalink: /Paper/Attention3/

toc: true
toc_sticky: true

date: 2024-05-21
# last_modified_at: 2024-05-20

author_profile: true
search: true
use_math: true
---

## Position-wise Feed-Forward Networks

<div style="display: flex; justify-content: space-around;">
    <img src="{{site.url}}/assets/images/posts_img/2024-05-21-1/FeedForward.png" alt="Attention" style="width: 100%;"/>
</div>

- 두 개의 선형 변환 사이에 ReLU 활성화 함수 사용
- 모든 위치에 동일하게 적용되지만 레이어마다 다른 파라미터 사용
- 입출력 차원 512, 내부 차원 2048

## Positional Encoding
<div align="center">
$$
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right)
$$

$$
PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right)
$$
</div>

- 순환 및 컨볼루션 없이 시퀀스 내 토큰 위치 정보를 추가해야 함
- 인코더와 디코더의 입력 임베딩에 위치 인코딩을 더함
- sine, cosine 함수의 서로 다른 주기를 가진 위치 인코딩 사용
- 각 차원이 특정 파장의 사인/코사인 함수에 대응
- 상대적 위치 정보를 효과적으로 학습할 수 있을 것으로 기대
- 학습된 위치 임베딩 대신 사인/코사인 함수를 사용한 이유는 더 긴 시퀀스에 대한 일반화 가능성 때문

## 레퍼런스

1. [Pytorch 공식 코드](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer) 
2. [Transformer 분석: Transformer의 Encoder 이해하기](https://moon-walker.medium.com/transformer-%EB%B6%84%EC%84%9D-2-transformer%EC%9D%98-encoder-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-1edecc2ad5d4)