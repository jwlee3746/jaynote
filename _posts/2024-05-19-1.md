---
title: "[논문 정리] Attention Is All You Need"
# excerpt: "본 포스팅은 샘플 포스트입니다." # 주요 내용

categories:
  - Paper
tags:
  - [NLP, Paper, Transformer]

permalink: /Paper/Attention Is All You Need/

toc: true
toc_sticky: true

# date: 2024-05-08
# last_modified_at: 2024-05-19

author_profile: true
search: true
use_math: true
---

## Abstract
이 논문은 기계 번역 및 자연어 처리 분야에서 새로운 neural network 아키텍처인 "**Transformer**"를 제안한다. 기존의 순환 신경망(RNN) 또는 컨볼루션 신경망(CNN)과 달리 Transformer는 전적으로 **어텐션 메커니즘**에 기반한다.

주요 내용:

- 기존 모델은 복잡한 RNN 또는 CNN 기반 인코더-디코더 구조를 사용했지만, Transformer는 **어텐션 메커니즘**만으로 구성된다.

- Transformer는 **병렬화**가 가능하고 학습 시간도 크게 단축할 수 있다.

- 실험 결과, Transformer 모델이 WMT 2014 영어-독일어, 영어-프랑스어 번역 테스크에서 기존 최고 모델보다 높은 BLEU 점수를 기록했다.


## Background
이 부분은 Transformer 모델이 기존 sequence-to-sequence 모델들과 다른 점을 설명하고 있습니다.
주요 내용은 다음과 같습니다:

기존 모델들(ByteNet, ConvS2S 등)은 CNN을 기반으로 입력/출력의 은닉 표현을 병렬로 계산합니다. 하지만 거리가 먼 위치 간의 의존성을 학습하기 어렵습니다.
Transformer는 Self-Attention 메커니즘을 통해 상수 시간 내에 모든 위치간 의존성을 계산할 수 있습니다. 다만 주의력 평균화로 인한 해상도 감소 문제가 있습니다.
Self-Attention은 단일 시퀀스 내 다른 위치 간의 관계를 모델링하는 메커니즘입니다. 다양한 자연어 처리 태스크에서 성공적으로 사용되었습니다.
Transformer는 RNN이나 CNN 대신 완전히 Self-Attention에 의존하는 첫 번째 sequence transduction 모델입니다.

요약하면, Transformer는 Self-Attention을 통해 위치 간 의존성을 상수 시간에 계산하고, 이를 통해 기존 모델들의 한계를 극복하고자 합니다.



<!-- ## 이미지
![jpg](../../assets/images/posts_img/2024-05-08-1/jpg.jpg)


## 동영상
{% include video id="cbuZfY2S2UQ" provider="youtube" %} -->
