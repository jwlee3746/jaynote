---
title: "[논문 리뷰] Attention Is All You Need"
excerpt: "자연어 분야의 핵심 논문 중 하나인 Attention Is All You Need을 리뷰한다. 핵심 개념인 어텐션 알고리즘과 모델 아키택처를 이해하고, 연산과 병렬화 관점에서 살펴본다." # 주요 내용

categories:
  - NLP
  - Paper
tags:
  - [NLP, Paper]

permalink: /Paper/Attention Is All You Need/

toc: true
toc_sticky: true

# date: 2024-05-08
# last_modified_at: 2024-05-19

author_profile: true
search: true
use_math: true
---

## Abstract
이 논문은 기계 번역 및 자연어 처리 분야에서 새로운 neural network 아키텍처인 "**Transformer**"를 제안한다. 기존의 순환 신경망(RNN) 또는 컨볼루션 신경망(CNN)과 달리 Transformer는 전적으로 **어텐션 메커니즘**에 기반한다.

주요 내용:

- 기존 모델은 복잡한 RNN 또는 CNN 기반 인코더-디코더 구조를 사용했지만, Transformer는 **어텐션 메커니즘**만으로 구성된다.

- Transformer는 **병렬화**가 가능하고 학습 시간도 크게 단축할 수 있다.

- 실험 결과, Transformer 모델이 WMT 2014 영어-독일어, 영어-프랑스어 번역 테스크에서 기존 최고 모델보다 높은 BLEU 점수를 기록했다.


## Background
이 부분은 Transformer 모델이 기존 sequence-to-sequence 모델들과 다른 점을 설명하고 있다:

| 특성 | 기존 모델| Transformer |
|---|---|---|
| 기본 구조 | Convolutional Neural Networks (CNNs)| Self-attention  |
| 연산 병렬화     | CNN 기반| Self-attention 기반|
| 멀리 떨어진 위치 간의 의존성 학습 | 어려움 (거리의 증가에 따라 연산량 증가)   | Multi-Head Attention으로 해결 |
| 장점 | N/A  | 위치 간 의존성을 효율적으로 계산하여 한계 극복 |
| 단점   | N/A  | 위치 가중치 평균화로 인한 해상도 감소 문제 발생  |

## Model Architecture
대부분의 최신 시퀀스 변환 모델은 인코더-디코더 구조를 따른다. 인코더는 입력 시퀀스 $\(x1,...,xn)\$ 를 연속된 표현 $\ z = (z1,...,zn) \$들로 매핑하고, 디코더는 이를 바탕으로 출력 시퀀스 $\(y1,...,ym)\$ 를 생성한다.

Auto-regressive (자기 회귀 모형): Time t 시계열(시퀀스)을 예측할 때 predictor로 t-1 데이터를 활용하는 모델. 디코더는 이전에 생성된 출력 심볼들을 추가 입력으로 사용하여 다음 심볼을 생성한다.
<div align="center">
$$ y_t = c + \sum_{i=1}^{p} \phi_i y_{t-i} + \epsilon_t $$
</div>

트랜스포머도 위 과정을 stacked self-attention과 point-wise & fully connected layer를 사용하여 해결한다.

### Encoder and Decoder Stacks

![Transformer]({{site.url}}/assets/images/posts_img/2024-05-19-1/transformer.png)

#### 인코더

![Encoder]({{site.url}}/assets/images/posts_img/2024-05-19-1/Encoder.png)

- 6개의 동일한 레이어로 구성됨
- 각 레이어는 두 개의 서브레이어로 이루어짐
- 멀티 헤드 셀프 어텐션
- 완전연결 피드포워드 네트워크
- 각 서브레이어 주변에 잔차 연결과 레이어 정규화가 있음
- 모든 레이어의 출력 차원은 512

#### 디코더

6개의 동일한 레이어로 구성됨
각 레이어는 세 개의 서브레이어로 이루어짐

멀티 헤드 셀프 어텐션
멀티 헤드 인코더-디코더 어텐션
완전연결 피드포워드 네트워크


셀프 어텐션에서 출력 위치가 입력 위치보다 이후인 경우는 마스킹 처리
잔차 연결과 레이어 정규화가 있음

#### 어텐션 메커니즘:

쿼리, 키, 값 벡터들의 가중합으로 계산됨
가중치는 쿼리와 키의 유사도에 의해 결정됨
멀티 헤드 어텐션은 여러 어텐션 레이어를 병렬로 수행함

요약하면 Transformer는 멀티 헤드 셀프 어텐션과 인코더-디코더 어텐션을 활용하며, 잔차 연결과 레이어 정규화로 안정적인 학습이 가능합니다.