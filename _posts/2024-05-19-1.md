---
title: "[논문 리뷰] Attention Is All You Need: Transformer, 코드로 이해하기"
excerpt: "자연어 분야의 핵심 논문 중 하나인 Attention Is All You Need을 리뷰한다. 핵심 개념인 어텐션 알고리즘과 모델 아키택처를 이해하고, 연산과 병렬화 관점에서 살펴본다." # 주요 내용

categories:
  - NLP
  - Paper
tags:
  - [NLP, Paper]

permalink: /Paper/Attention Is All You Need/

toc: true
toc_sticky: true

# date: 2024-05-08
last_modified_at: 2024-05-20

author_profile: true
search: true
use_math: true
---

## Abstract
이 논문은 기계 번역 및 자연어 처리 분야에서 새로운 neural network 아키텍처인 "**Transformer**"를 제안한다. 기존의 순환 신경망(RNN) 또는 컨볼루션 신경망(CNN)과 달리 Transformer는 전적으로 **어텐션 메커니즘**에 기반한다.

주요 내용:

- 기존 모델은 복잡한 RNN 또는 CNN 기반 인코더-디코더 구조를 사용했지만, Transformer는 **어텐션 메커니즘**만으로 구성된다.

- Transformer는 **병렬화**가 가능하고 학습 시간도 크게 단축할 수 있다.

- 실험 결과, Transformer 모델이 WMT 2014 영어-독일어, 영어-프랑스어 번역 테스크에서 기존 최고 모델보다 높은 BLEU 점수를 기록했다.


## Background
이 부분은 Transformer 모델이 기존 sequence-to-sequence 모델들과 다른 점을 설명하고 있다:

| 특성 | 기존 모델| Transformer |
|---|---|---|
| 기본 구조 | Convolutional Neural Networks (CNNs)| Self-attention  |
| 연산 병렬화     | CNN 기반| Self-attention 기반|
| 멀리 떨어진 위치 간의 의존성 학습 | 어려움 (거리의 증가에 따라 연산량 증가)   | Multi-Head Attention으로 해결 |
| 장점 | N/A  | 위치 간 의존성을 효율적으로 계산하여 한계 극복 |
| 단점   | N/A  | 위치 가중치 평균화로 인한 해상도 감소 문제 발생  |

## Model Architecture
대부분의 최신 시퀀스 변환 모델은 인코더-디코더 구조를 따른다. 인코더는 입력 시퀀스 $\(x1,...,xn)\$ 를 연속된 표현 $\ z = (z1,...,zn) \$들로 매핑하고, 디코더는 이를 바탕으로 출력 시퀀스 $\(y1,...,ym)\$ 를 생성한다.

Auto-regressive (자기 회귀 모형): Time t 시계열(시퀀스)을 예측할 때 predictor로 t-1 데이터를 활용하는 모델. 디코더는 이전에 생성된 출력 심볼들을 추가 입력으로 사용하여 다음 심볼을 생성한다.
<div align="center">
$$ y_t = c + \sum_{i=1}^{p} \phi_i y_{t-i} + \epsilon_t $$
</div>

트랜스포머도 위 과정을 stacked self-attention과 point-wise & fully connected layer를 사용하여 해결한다.

### Encoder and Decoder Stacks

<!-- ![Transformer](../assets/images/posts_img/2024-05-19-1/transformer.png) -->

#### 인코더

<div style="display: flex; justify-content: space-around;">
    <img src="{{site.url}}/assets/images/posts_img/2024-05-19-1/EncoderLayer.png" alt="Image 2" style="width: 50%;"/>
    <img src="{{site.url}}/assets/images/posts_img/2024-05-19-1/Encoder.png" alt="Image 1" style="width: 50%;"/>
</div>

- 각 인코더는 2개의 레이어(muti-head self attention, fully-connected feed-forward)로 이루어짐
- 각 서브 레이어에 잔차 연결과 레이어 정규화가 있음: stack 구조로 인한 그래디언트 소실 방지
- 모든 서브 레이어의 출력 차원은 d_model(default=512) 값으로 고정

```python
class Encoder(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size, max_len, dropout=0.1):
        super(Encoder, self).__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(input_vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, dropout, max_len)
        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])

    def forward(self, src, src_mask):
        x = self.embedding(src) * torch.sqrt(torch.tensor(self.d_model).float())
        x = self.pos_encoding(x)
        for layer in self.layers:
            x = layer(x, src_mask)
        return x

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(EncoderLayer, self).__init__()
        self.multihead_attention = MultiHeadAttention(d_model, num_heads)
        self.positionwise_feedforward = PositionwiseFeedforward(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm1 = nn.LayerNorm(d_model)
        self.layer_norm2 = nn.LayerNorm(d_model)

    def forward(self, src, src_mask):
        x = src
        # Multi-Head Attention
        x = self.layer_norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)) # (query, key, value)
        # Positionwise Feedforward
        x = self.layer_norm2(x + self._ff_block(x))
        return x
```

#### 디코더

6개의 동일한 레이어로 구성됨
각 레이어는 세 개의 서브레이어로 이루어짐

멀티 헤드 셀프 어텐션
멀티 헤드 인코더-디코더 어텐션
완전연결 피드포워드 네트워크


셀프 어텐션에서 출력 위치가 입력 위치보다 이후인 경우는 마스킹 처리
잔차 연결과 레이어 정규화가 있음

#### 어텐션 메커니즘:

쿼리, 키, 값 벡터들의 가중합으로 계산됨
가중치는 쿼리와 키의 유사도에 의해 결정됨
멀티 헤드 어텐션은 여러 어텐션 레이어를 병렬로 수행함

요약하면 Transformer는 멀티 헤드 셀프 어텐션과 인코더-디코더 어텐션을 활용하며, 잔차 연결과 레이어 정규화로 안정적인 학습이 가능합니다.